= Automate your deployment with Tekton
So far we exercised what we call the Inner-loop development workflow where you essentially iteratively do code, build, test, debug, deploy in a local environment (or a Dev Workspace in our case). But how about the build and deployment after we push our code to a Git repository?

In this section you are going to learn how to automate the build and deployment of your Integrations using a Cloud Native https://www.redhat.com/en/topics/devops/what-is-ci-cd[CI/CD technology^] called https://tekton.dev[Tekton^]. Tekton is available on Openshift through the Openshift Pipelines Operator which is already present on our cluster.

== Automating the Camel K deployment 
We learned that Camel K does a great job containerizing our Integrations and deploying on Openshift. Now we are going to learn how to integrate it with Openshift Pipelines so you can leverage CI/CD tools to delivery integrations following corporate standards.

Lets deploy the `Order Connector` integration service we developed in `module-02`. 

[NOTE]
====
If you have not created a `ConfigMap` for the `Order Connector` integration during module-02, please do it before triggering the Pipeline run.

Run the following command in your DevWorkspace Terminal
[source,sh,role="copypaste",subs=attributes+]
....
oc create configmap order-connector-config --from-file=$PROJECT_SOURCE/module-02/order-connector/application.properties -n {user}-camel

# in case you choose one of the other services
# customer-connector
oc create configmap customer-connector-config --from-file=$PROJECT_SOURCE/module-02/customer-connector/application.properties -n {user}-camel

# customer-service
oc create configmap customer-service-config --from-file=$PROJECT_SOURCE/module-02/customer-service/application.properties -n {user}-camel

oc create configmap customer-openapi-spec --from-file=$PROJECT_SOURCE/module-02/customer-service/customer-openapi-spec.json -n {user}-camel
....
====

A Camel K Pipeline has been already created for you, to trigger it go to the **Pipelines** view on Openshift Console (Developer Perspective), click on `camel-k-pipeline`, then select `Actions` -> `Start`. Then enter the parameters as follow.

[IMPORTANT]
====
You may have the integration services already deployed/running inside the `{user}-camel` project/namespace. They were deployed as part of module-02 using `kamel run` CLI tool. Please check if they are deployed and remove them before you proceed. 

To check there is any integration already deployed ad running inside `{user}-camel` project execute this command inside your DevWorkspace Terminal
[source,sh,role="copypaste",subs=attributes+]
....
kamel get -n {user}-camel
....

To remove deployed integrations

[source,sh,role="copypaste",subs=attributes+]
....
#to remove specific integration
kamel delete order-connector -n {user}-camel
kamel delete customer-connector -n {user}-camel
....
====

.Pipeline parameters
[%autowidth, cols="d,l"]
|===
|Name|Value

|repo-url|leave as is
|repo-branch|main (if you didn't complete module-02, enter solution)
|filename|module-02/order-connector/order-connector.camel.yaml
|traits|leave it as is
|dependencies|-d camel:http -p configmap:order-connector-config
|Workspaces -> shared-data|PersistentVolumeClaim -> PVC source-code (select from the drop-down list)
|===

See these steps in action here.

image::module04/camel-k-pipeline-run.gif[]

After the `Pipeline run` completes successfully you should be able to check a new Camel POD running in the topology view. Click on the POD and see the Container logs to check the integration startup as follows.

image::module04/camel-k-pod-topology.gif[]

[TIP]
====
In this exercise we triggered the Pipeline manually, but in practice you would have a **Webhook** in place so the Pipeline can run **continuously** every time a new change gets pushed to the integration code Git repository. In case you are interested in the Pipeline Triggering capability check https://www.redhat.com/en/blog/guide-to-openshift-pipelines-part-6-triggering-pipeline-execution-from-github[this tutorial^] out for reference.
====

[NOTE]
====
You can use this same Pipeline to build and deploy all other integrations we created previously by just passing the correct `filename` and `dependencies` parameters.

 * for the `customer-service` the parameters are:

.Pipeline parameters
[%autowidth, cols="d,l"]
|===
|Name|Value

|filename|module-02/customer-service/customer-service.camel.yaml
|dependencies|-d camel:platform-http -d mvn:org.postgresql:postgresql:42.7.3 -p configmap:customer-service-config --open-api configmap:customer-openapi-spec
|===

 * for the `customer-connector` the parameters are:

.Pipeline parameters
[%autowidth, cols="d,l"]
|===
|Name|Value

|filename|module-02/customer-connector/customer-connector.camel.yaml
|dependencies|-d camel:http -p configmap:customer-connector-config
|===
====

== Exporting Camel-K integration to Java Maven Project
Another option available in the Apache Camel *toolset* is the ability to **export** your yaml Route to a traditional **Java Maven Project**. This can be achieved by using the https://camel.apache.org/manual/camel-jbang.html[Camel JBang app^] which is already available in your DevWorkspace. 

[NOTE]
====
Having your Camel-K integration exported as a Maven project opens up room for all sorts of customization and more fine-grained control on how to build images from your source code. Camel K makes it easy to deploy from YAML, but if your use case is more complex or requires more control over the release process, you may want to consider to maintain a Java Project. You will still be able to use Kaoto to work on your route design.
====

To export the `customer-connector` integration developed in module-02 as a Java Maven Project to run on Quarkus Runtime, execute the following command in your DevWorkspace terminal.

[source,bash,role="copypaste",subs=attributes+]
....
cd $PROJECT_SOURCE
mkdir module-04 && cd module-04

jbang run '-Dcamel.jbang.version=4.6.0-SNAPSHOT' camel@apache/camel export \
--gav=com.redhat.lab:customer-connector:1.0.0-SNAPSHOT \
--runtime=quarkus \
--directory=customer-connector \
../module-02/customer-connector/*.camel.yaml
....

[TIP]
====
More details on the capabilities available in the Camel JBang please see the https://camel.apache.org/manual/camel-jbang.html[project docs^].
====

The command may take a couple of minutes when you first run it as `jbang` needs to download dependencies from the Maven repository. Please, wait until the commands completes as follows.

image::module04/camel-jbang-export-to-quarkus.gif[]

As you can see, you get a standard Java Maven Project with this JBang command. 
Let's do one change to the generated code so it can connect to our Kafka service. Using your DevWorkspace, open the `application.properties` file located under `module-04/customer-connector/src/main/resources/` and replace its content with: 

[source,properties,role="copypaste",subs=attributes+]
....
quarkus.native.resources.includes=
camel.main.routes-include-pattern=camel/customer-connector.camel.yaml

# Prod configuration
%prod.kafka.brokers=kafka-kafka-bootstrap.user1-globex.svc.cluster.local:9092
%prod.kafka.securityProtocol=SASL_PLAINTEXT
%prod.kafka.saslMechanism=SCRAM-SHA-512
%prod.kafka.saslJaasConfig=org.apache.kafka.common.security.scram.ScramLoginModule required username='globex' password='globex';

%prod.customer-service.url=customer-service.user1-camel.svc.cluster.local:80

# Dev configuration
%dev.kafka.brokers=localhost:9092
%dev.customer-service.url=localhost:8080
....

Now you can build the app locally by executing the following command in a Terminal:

[source,bash,role="copypaste",subs=attributes+]
....
cd $PROJECT_SOURCE/module-04/customer-connector
mvn package
....

After building the app locally, you can run it locally by executing:

[source,bash,role="copypaste",subs=attributes+]
[subs=normal]
....
cd $PROJECT_SOURCE/module-04/customer-connector
java -jar target/quarkus-app/quarkus-run.jar
....

[IMPORTANT]
====
The `customer-connector` integration depends on the `customer-service`, so before running the following command locally make sure the `customer-service` is deployed.
====

You should see a lot of log entries as the Camel route start processing the Kafka record stream.

Okay, now lets prepare our Quarkus app to be ready for CI/CD. Execute the following command in your DevWorkspace Terminal:

[source,bash,role="copypaste",subs=attributes+]
....
cd $PROJECT_SOURCE/module-04/customer-connector
mvn quarkus:add-extension -Dextensions='quarkus-openshift'
....

expect an output like this:

[source,bash]
....
[INFO] Scanning for projects...
[INFO] 
[INFO] -----------------< com.redhat.lab:customer-connector >------------------
[INFO] Building customer-connector 1.0.0-SNAPSHOT
[INFO]   from pom.xml
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- quarkus:3.9.4:add-extension (default-cli) @ customer-connector ---
[INFO] Looking for the newly published extensions in registry.quarkus.io
[INFO] [SUCCESS] âœ…  Extension io.quarkus:quarkus-openshift has been installed
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  8.233 s
[INFO] Finished at: 2024-04-26T22:39:11Z
[INFO] ------------------------------------------------------------------------
....

[IMPORTANT]
====
This `quarkus-openshift` extension is needed so the pipeline can generate all the deployment resources (`Deployment`, `Service`, `Route`, `etc`) required to deploy to Openshift. All these resources will be automatically generated by the Quarkus Maven plugin through this extension.
====

Now commit and push these changes to your Git repo!

image::module04/quarkus-app-commit-push.gif[]

Now lets build and deploy our Camel Quarkus app using Openshift Pipelines. Go to the **Pipelines** view on Openshift Console (Developer Perspective), click on `camel-java-pipeline`, then select `Actions` -> `Start`. All the parameters has default values properly set to build and deploy the `customer-connector` integration just exported by JBang earlier in this module. You just need to select the `shared-data` workspace and set it to `PVC source-code`. Click `Start` button to trigger the Pipeline and follow its execution like shown bellow.

image::module04/java-pipeline-run.gif[]

With that we conclude this module-04! Congratulations!